# graph.py
import os
import asyncio
import httpx
from typing import TypedDict, Dict
from dotenv import load_dotenv
from langgraph.graph import StateGraph, END

load_dotenv()

# API Keys and Endpoint configuration
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
GROQ_API_KEY = os.getenv("GROQ_API_KEY")
# Using the secure Caddy proxy address defined in your install.sh and Caddyfile
OLLAMA_BASE_URL = os.getenv("OLLAMA_BASE_URL", "https://guardian.local:11435")

# The 5-Pillar Audit logic defined in your README.md
SYSTEM_PROMPT = """You are an expert code reviewer. Perform a 5-pillar audit on the code:
1. Security: Identify vulnerabilities or data leaks.
2. Performance: Spot bottlenecks or inefficient algorithms.
3. Readability: Evaluate naming conventions and clarity.
4. Maintainability: Assess modularity and technical debt.
5. Architectural Alignment: Ensure best practices for the language.

Format your response in clear markdown with sections for each pillar."""

class AgentState(TypedDict):
    code: str
    reviews: Dict[str, str]
    user_message: str

async def call_openai(code: str, user_message: str) -> str:
    prompt = f"Additional Instructions: {user_message}\n\nCode:\n{code}" if user_message else code
    async with httpx.AsyncClient(timeout=90, verify=False) as client:
        # verify=False is used because of the self-signed certs generated by your install.sh
        response = await client.post(
            "https://api.openai.com/v1/chat/completions",
            headers={"Authorization": f"Bearer {OPENAI_API_KEY}"},
            json={
                "model": "gpt-4o",
                "messages": [
                    {"role": "system", "content": SYSTEM_PROMPT},
                    {"role": "user", "content": prompt},
                ],
            },
        )
        response.raise_for_status()
        return response.json()["choices"][0]["message"]["content"]

async def call_groq(code: str, user_message: str) -> str:
    prompt = f"Additional Instructions: {user_message}\n\nCode:\n{code}" if user_message else code
    async with httpx.AsyncClient(timeout=90, verify=False) as client:
        response = await client.post(
            "https://api.groq.com/openai/v1/chat/completions",
            headers={"Authorization": f"Bearer {GROQ_API_KEY}"},
            json={
                "model": "llama-3.3-70b-versatile",
                "messages": [
                    {"role": "system", "content": SYSTEM_PROMPT},
                    {"role": "user", "content": prompt},
                ],
            },
        )
        response.raise_for_status()
        return response.json()["choices"][0]["message"]["content"]

async def call_ollama(model: str, code: str, user_message: str) -> str:
    prompt = f"Additional Instructions: {user_message}\n\nCode:\n{code}" if user_message else code
    # Longer timeout for local models as per your previous implementation
    custom_timeout = httpx.Timeout(300.0, connect=10.0, read=300.0)
    async with httpx.AsyncClient(timeout=custom_timeout, verify=False) as client:
        try:
            response = await client.post(
                f"{OLLAMA_BASE_URL}/api/chat",
                json={
                    "model": model,
                    "messages": [
                        {"role": "system", "content": SYSTEM_PROMPT},
                        {"role": "user", "content": prompt},
                    ],
                    "stream": False,
                },
            )
            response.raise_for_status()
            return response.json()["message"]["content"]
        except Exception as e:
            return f"Error calling {model} at {OLLAMA_BASE_URL}: {str(e)}"

async def run_all_reviews(code: str, user_message: str) -> Dict[str, str]:
    # Running models concurrently to support the 'Parallel Auditing' feature
    results = await asyncio.gather(
        call_openai(code, user_message),
        call_groq(code, user_message),
        call_ollama("deepseek-r1:1.5b", code, user_message),
        call_ollama("llama3.2", code, user_message),
        return_exceptions=True,
    )

    # These keys match the AVAILABLE_MODELS mapping in your main.py
    model_ids = ["GPT-4o", "Grok 3", "DeepSeek R1 (Local)", "Llama 3.2 (Local)"]
    reviews = {}
    for model_id, result in zip(model_ids, results):
        if isinstance(result, Exception):
            reviews[model_id] = f"Error: {str(result)}"
        else:
            reviews[model_id] = result
    return reviews

async def code_reviewer(state: AgentState) -> dict:
    reviews = await run_all_reviews(state["code"], state.get("user_message", ""))
    return {"reviews": reviews}

def create_graph():
    # Standard LangGraph workflow construction
    workflow = StateGraph(AgentState)
    workflow.add_node("reviewer", code_reviewer)
    workflow.set_entry_point("reviewer")
    workflow.add_edge("reviewer", END)
    return workflow.compile()